{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is some NLP on the comment corpus collected from regulations.gov for the proposal EPA–R08–OAR–2015–0463.\n",
    "\n",
    "I have followed \"Corpora_and_Vector_Spaces\" gensim tutorial https://radimrehurek.com/gensim/tut1.html\n",
    "\n",
    "In this example we remove common word strings like \"of\", \"the\", etc and also remove words that only appear once (across all comments).  The remaining words make up the dictionary.  Each comment has a collection of vectors.  The vectors are stored as an ordered pair (a,b) where \"a\" is the ID of the word and \"b\" is the number of times that word appears in the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "import pickle\n",
    "import glob\n",
    "from pprint import pprint  # pretty-printer\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We use pickle to retrive the file.  We have two files; the one labled \"clean\" was created in the \"Corpus_Cleaning\" notebook and removed non-word code like \"\\n\".\n",
    "\n",
    "It seems to make the corpus smaller, but we will keep an eye out in case keeping the \"\\n\"'s etc, may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPA–R08–OAR–2015–0463.pkl\r\n",
      "EPA–R08–OAR–2015–0463.pkl_clean.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls *.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elliegrano/codes/NLP/blog/data-collection\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = glob.glob('/Users/elliegrano/codes/NLP/blog/data-collection/*_clean.pkl')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the dictionary back from the pickle file.\n",
    "\n",
    "corpus = pickle.load( open( fname, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EPA\\xe2\\x80\\x93R08\\xe2\\x80\\x93OAR\\xe2\\x80\\x932015\\xe2\\x80\\x930463'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docID is the ID of the EPA proposal\n",
    "docID=corpus.keys()[0]\n",
    "docID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These are the comments loaded into the corpus\n",
    "# docID is the EPA proposal ID number\n",
    "# corpus[docID] is a dictionary with structure the comment ID : the comment\n",
    "\n",
    "comments = corpus[docID]\n",
    "# comments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utah's DAQ & AQ B deserve deference.   Utah has complied with the 70 recommendations of the Grand Canyon Visibility Transport Commission, including submitting a 309 SIP in 2003, 5 years before agencies complying with 308 provisions.  Utah has submitted all the required sulfur dioxide Milestone and Backstop Trading Program reports, has revised the program frequently to accommodate litigation and various members leaving the program.  All reports & updates have been submitted in a timely fashion, following final action by Utah's AQB after notice and public comment.  This was at very considerable investment in staff time, study by AQB members and meaningful contributions by the public.  Under the provisions of Utah's 309 SIP, additional pollution controls were applied to several units at Hunter & Huntington which reduced PM, SO2 and NOx.    We reached our 2018 SO2 reduction target years early, in 2010, and SO2 has continued to decline.   Analysis of monitoring values after instillation of these RH SIP required controls indicate that the model had substantially over-predicted visibility improvements from NOx reductions.  Rather than requiring expensive SCR for minimal visibility improvement, political & financial capital is better spent determining what will actually improve visibility.    This extensive & extended comment & hearing schedule has served to increase polarization in our state and divert attention from creating solutions to our air quality problems that are effective & politically palatable.   If one is working for attainable solutions, litigation & over-long delay of decisions is not useful.  Please approve Utah's June 4 & Oct 20 2015 submissions.    Thank you for your attention to these comments.  \n"
     ]
    }
   ],
   "source": [
    "# example of a comment\n",
    "one_comment = comments[comments.keys()[1]]\n",
    "print(one_comment)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'values', u'after', u'instillation', u'these', u'rh', u'sip', u'required', u'controls', u'indicate', u'that', u'model', u'had', u'substantially', u'over', u'predicted', u'visibility', u'improvements', u'from', u'nox', u'reductions']\n",
      "[u'values', u'after', u'these', u'rh', u'sip', u'required', u'controls', u'indicate', u'that', u'model', u'had', u'substantially', u'over', u'predicted', u'visibility', u'improvements', u'from', u'nox', u'reductions', u'rather']\n"
     ]
    }
   ],
   "source": [
    "# remove common words and make all words lowercase\n",
    "\n",
    "stoplist = set('s by at & i we if has is was were for a of the and to in'.split())\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # removes punctuation\n",
    "\n",
    "# Q: maybe include this in corpus cleaning or could this replace the work done in corpus cleaning\n",
    "\n",
    "raw_texts = [[word for word in tokenizer.tokenize(value.lower()) if word not in stoplist]\n",
    "         for key, value in comments.iteritems()]\n",
    "\n",
    "print(raw_texts[1][110:130]) # all words not in stoplist in comment 1\n",
    "# (printing only some from the middle)\n",
    "\n",
    "\n",
    "# remove words that appear only once\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in raw_texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "texts = [[token for token in text if frequency[token] > 1] for text in raw_texts]\n",
    "\n",
    "print(texts[1][110:130]) # all words not in stoplist in comment 1\n",
    "# that ALSO appear in other comments in the corpus \n",
    "# (printing only some from the middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next is just showing which words were removed in comment 1 in \"texts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200: number of words in comment 1 not in stoplist   \n",
      "194: same, removing words that do not appear in other comments in the corpus (will not be in the dictionary)\n"
     ]
    }
   ],
   "source": [
    "print(str(len(raw_texts[1])) + \": number of words in comment 1 not in stoplist   \")\n",
    "print( str(len(texts[1]))+\": same, removing words that do not appear in other comments in the corpus (will not be in the dictionary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112  instillation\n",
      "157  polarization\n",
      "160  divert\n",
      "172  politically\n",
      "173  palatable\n",
      "176  attainable\n"
     ]
    }
   ],
   "source": [
    "# words in comment 1 that did not appear in any other comment\n",
    "for i, w in enumerate(raw_texts[1]):\n",
    "    if (w not in texts[1]):\n",
    "        print(str(i) + '  ' + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utah: 5594\n",
      "instillation: 1\n"
     ]
    }
   ],
   "source": [
    "# number of times a word appears in the collection of comments\n",
    "print(\"utah: \" + str(frequency['utah']))\n",
    "print(\"instillation: \" + str(frequency['instillation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632\n",
      "0\n",
      "2\n",
      "4\n",
      "8\n",
      "10\n",
      "11\n",
      "14\n",
      "19\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "# comments where the word \"existing\" occurs\n",
    "print(frequency['existing'])\n",
    "for i, c in enumerate(texts):\n",
    "    word='existing'\n",
    "    if (word in c):\n",
    "        print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we store the dictionary using corpora from gensim.  This stores the dictionary in RAM which we will change later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(22734 unique tokens: [u'fawn', u'11546', u'11545', u'11548', u'11549']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('/tmp/dstore.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'fawn', 22167),\n",
       " (u'11546', 5366),\n",
       " (u'11545', 5367),\n",
       " (u'11548', 5368),\n",
       " (u'11549', 5369),\n",
       " (u'localized', 5372),\n",
       " (u'3483', 5373),\n",
       " (u'sprague', 5374),\n",
       " (u'19395', 5375),\n",
       " (u'refunding', 5378),\n",
       " (u'stipulate', 5388),\n",
       " (u'appropriation', 5389),\n",
       " (u'rawhide', 5566),\n",
       " (u'bringing', 5392),\n",
       " (u'wooded', 341)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(dictionary.token2id)\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "take(15, dictionary.token2id.iteritems())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The fact that we are storing a lot of numbers $\\uparrow $  may be a concern.  On the otherhand the proposal itself has numbers in it describing sections or other proposals, so we may want to keep at least some of the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1173, 1), (1472, 2), (4933, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = tokenizer.tokenize((\"Years and years of truely serious litigation.\").lower())\n",
    "new_vec = dictionary.doc2bow(new_doc)\n",
    "print(new_vec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serious\n",
      "years\n",
      "litigation\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[1173])\n",
    "print(dictionary[1472])\n",
    "print(dictionary[4933])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The word \"years\" appears twice, \"litigation\" once, and \"serious\" once.  Ignores the punctuation, case, and flagged words.  Apparently \"truely\" doesn't appear more than once in the collection of comments, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155: unique words in comment 1 that are in the dicionary\n"
     ]
    }
   ],
   "source": [
    "new_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/tmp/dstore.mm', new_corpus)  # store to disk, for later use\n",
    "# for c in new_corpus:\n",
    "#     print(c)\n",
    "# print(new_corpus[0:1])\n",
    "print(str(len(new_corpus[1]))+\": unique words in comment 1 that are in the dicionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4933\n",
      "155\n",
      "1516\n"
     ]
    }
   ],
   "source": [
    "# count=0\n",
    "# for vector in new_corpus:  # load one vector into memory at a time\n",
    "#     if (0<count<3):\n",
    "# #         print(vector)\n",
    "#         print(len(vector))\n",
    "#         count+=1\n",
    "\n",
    "for i in range(0,3):\n",
    "    print(len(new_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "I want to organize the comments by distance:\n",
    "Perhaps determine which ones share words.  \n",
    "\n",
    "new_corpus[1] and new_corpus[2] both share the word 'under' (#26), 1 time for comment1 and 7 times for comment2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "shared = []\n",
    "for i in range(0,len(new_corpus[1])):\n",
    "    for j in range(0,len(new_corpus[2])):\n",
    "        if(new_corpus[1][i][0]==new_corpus[2][j][0]):\n",
    "            shared.append(new_corpus[1][i][0])\n",
    "#             print(new_corpus[1][i][0])\n",
    "            \n",
    "print(shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Word to Vec should do this stuff for me though.\n",
    "I'll keep going through the gensim tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we make it so the entire corpus is not stored in RAM, but read in one comment at a time.  I'm not sure that this is necessary since we already store the original corpus.  Perhaps this is useful during corpus collection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for key, comment in comments.iteritems():\n",
    "            yield dictionary.doc2bow(tokenizer.tokenize(comment.lower()))\n",
    "\n",
    "            \n",
    "#  raw_texts = [[word for word in tokenizer.tokenize(value.lower()) if word not in stoplist]\n",
    "#          for key, value in comments.iteritems()]\n",
    "\n",
    "# texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "# >>>          for document in documents]\n",
    "# >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x10cda34d0>\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4933, 155, 1516, 9, 4945, 214, 414, 419, 531, 231, 20904, 2110, 357, 506, 2836, 375, 652, 253, 233, 5001, 336, 586, 425]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# checking these produce the same dictionary\n",
    "\n",
    "len_1 = []\n",
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    len_1.append(len(vector))\n",
    "print(len_1)\n",
    "\n",
    "len_2 = []\n",
    "for i in range(0,len(new_corpus)):\n",
    "    len_2.append(len(new_corpus[i]))\n",
    "print(len_1==len_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we will determine which comments are near each other using bag of words in \"Topics and Transformations\" continuing the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
